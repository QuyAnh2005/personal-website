---
title: 'MoD: A Distribution-Based Approach for Merging Large Language Models'
description: 'A distribution-based approach for merging large language models.'
date: '2024-11-01'
conference: "LLM Merging Competition at NeurIPS 2024"
link: "https://openreview.net/forum?id=v2tZ9bNcS5"
tags: ["Machine Learning", "Large Language Model"]
---


Merging large language models (LLMs) traditionally involves combining model weights, which often distorts the models’ specialized capabilities. We introduce **Mixture of Distributions (MoD)**, a new approach that operates on the *output probability distributions* instead of model parameters. MoD preserves key distributional properties of each model and significantly outperforms conventional merging methods on various math benchmarks. We release our code and experimental materials at [https://github.com/knovel-eng/mod](https://github.com/knovel-eng/mod).

---

## Introduction
Recent open-source LLMs often specialize in specific tasks—e.g., coding, medical text, or mathematical reasoning. While fine-tuning these models for each domain is powerful, deploying multiple specialized models can be impractical. Weight-based merging methods partially address this challenge but risk “averaging out” critical features of each model.

We propose **MoD (Mixture of Distributions)**, a technique that combines *probability distributions* of LLMs. Instead of interpolating parameters, MoD preserves the high-density regions of each distribution, keeping the best of each specialized model. Our experiments on math-centric benchmarks show MoD’s consistent gains over popular merging techniques like Linear, Task-Arithmetic, TIES, DARE, and SLERP.

---

## Method
### Key Idea
Conventional weight-averaging yields a single set of parameters:
$$
\theta = \alpha \theta_1 + (1 - \alpha) \theta_2
$$
This can create spurious density peaks—points where neither original model had high probability. Instead, **MoD** merges the *output distributions*:
$$
p_{\theta}(x) = \alpha p_{\theta_1}(x) + (1 - \alpha) p_{\theta_2}(x)
$$
Here, $p_{\theta_i}(x)$ is the likelihood that model $i$ assigns to token $x$. The parameter $\alpha$ governs how much each model contributes. Critically, this mixture form preserves each model’s distributional structure, preventing catastrophic forgetting or over-smoothing.

![Method](/images/content/research/20241010_mod_pipeline.png)

### Implementation
1. **Identify Specialized Models**: Choose two (or more) fine-tuned LLMs with complementary strengths.
2. **Set Mixture Weights**: Assign $\alpha$ (e.g., 0.9 for a general model, 0.1 for a math model) or optimize $\alpha$ based on validation performance.
3. **Combine Outputs**: During inference, compute token probabilities as a weighted sum of each model’s logits. Sample or pick the highest-probability token from $p_{\theta}(x)$.

This distribution-level merging is lightweight: MoD only needs each model’s *inference outputs* rather than training or re-initialization.

---

## Results
We tested MoD on 12 math-focused benchmarks (e.g., GSM8K, MATH, ASDiv, MMLU STEM). We merged pairs of Qwen2.5 Instruct and Qwen2.5 Math Instruct models (1.5B and 7B parameters). MoD consistently yielded higher accuracy than baseline merging approaches:

![Results](/images/content/research/20241010_mod_results.png)

- **Qwen2.5-1.5B Merging**: MoD outperformed Linear, Task-Arithmetic, TIES, DARE, and SLERP by large margins. For example:
  - GSM8K: MoD scored **74.5%** vs. 51.5% (DARE) and 47.3% (SLERP).
  - MATH: MoD reached **55.8%**, doubling the best baseline score.
- **Qwen2.5-7B Merging**: MoD maintained its lead:
  - GSM8K: **92.4%** (MoD) vs. 91.9% (Linear) and 90.9% (DARE).
  - MATH: **75.4%** (MoD) vs. 72.2% (SLERP).

These results show that combining *probability distributions* preserves each model’s specialized knowledge while forming a stronger multitask model. In the LLM Merging Competition, MoD demonstrated both high accuracy and efficiency, requiring only modest GPU memory during inference.

---

*Interested readers can find the complete code and examples in our [GitHub repository](https://github.com/knovel-eng/mod).*