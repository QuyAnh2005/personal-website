---
title: "wav2graph: A Framework for Supervised Learning Knowledge Graph from Speech"
description: "A framework for supervised learning knowledge graph from speech."
date: "2024-08-08"
journal: "arXiv"
authors: ["Khai Le-Duc", "Quy-Anh Dang", "Tan-Hanh Pham", "Truong-Son Hy"]
link: "https://arxiv.org/abs/2408.04174"
tags: ["Machine Learning", "Knowledge Graph"]
---


Knowledge graphs (KGs) provide structured, interconnected data that can enhance the reasoning ability of large language models and search engines. Existing KGs, however, mostly rely on text sources and overlook other modalities like speech. **wav2graph** is introduced as the first framework that *supervises* KG construction and training *directly* from speech. Our approach (1) builds KGs from transcribed utterances and named entities, (2) converts the resulting graph into embeddings, and (3) trains graph neural networks (GNNs) for node classification and link prediction tasks. Empirical results on both human transcripts and ASR transcripts demonstrate the effectiveness of wav2graph, even under high word error rates (WERs). Code, data, and models are publicly available.

---

## Introduction
Knowledge graphs (KGs) encode real-world facts as nodes (entities) and edges (relations), enabling efficient storage and retrieval of large, interconnected data. They play a vital role in search engines and question-answering systems by improving context and reasoning capabilities. Yet, KGs are predominantly built from textual data, overlooking audio sources.

**wav2graph** addresses this gap by constructing *voice-based* KGs using spoken utterances and named entities, then training GNNs for classification and prediction on these KG structures. Our KG includes both *utterance nodes* (what was spoken) and *named_entity nodes* (extracted from text). We evaluate node classification (to predict node types) and link prediction (to determine relationships among nodes), leveraging a variety of speech and text embeddings.

---

## Method
![Method](/images/content/research/20241010_wav2graph_pipeline.png)
1. **Audio Transcription**  
   We convert speech signals into text using automatic speech recognition (ASR). We explore both monolingual and multilingual acoustic pre-training.

2. **KG Construction**  
   - From transcribed text, we extract named entities (NEs) via gold-standard labels.  
   - We form a graph where utterance nodes link to NE nodes that appear in those utterances.

3. **Node Representation**  
   - Node embeddings are derived from large pre-trained encoders (e.g., multilingual-e5, PhoBERT) or decoder-based embeddings (Qwen2).  
   - We also explore random embeddings to compare performance.

4. **GNN Training**  
   - We train graph neural networks (GNNs)—SAGE, GCN, GAT, SuperGAT—to perform two tasks:  
     **Node classification** predicts a node attribute (e.g., utterance vs. named_entity type).  
     **Link prediction** predicts edges between node pairs.  
   - We compare inductive and transductive learning setups on both human and ASR transcripts.

---

## Results
- **Human Transcript**  
  - **Node Classification**: Embedding quality is crucial. SAGE with BERT-based text embeddings achieves near-perfect metrics in inductive learning.  
  - **Link Prediction**: GCN with contextual embeddings excels, highlighting the role of local graph structure.

- **ASR Transcript**  
  - Despite 28–29% WER, our GNNs maintain competitive performance. Random embeddings even rival advanced text embeddings in some transductive settings.  
  - For node classification, **multilingual** acoustic pre-training plus **multilingual** embeddings yields strong results.  
  - For link prediction, GCN again demonstrates robust performance, underscoring its effectiveness in capturing node-to-node relationships even with noisy transcripts.

**Conclusion**  
wav2graph highlights a novel direction: merging speech-derived data into knowledge graphs and training them with GNNs. Surprisingly, noisy transcripts (high WER) do not always hamper performance—graph-structured embeddings help mitigate ASR errors. Future research can further improve zero-shot adaptation and examine more languages for large-scale spoken KG construction.