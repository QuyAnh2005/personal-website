---
title: "Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn’t"
description: "A study exploring reinforcement learning techniques to boost reasoning capabilities in small large language models."
date: "2025-03-25"
authors: ["Quy-Anh Dang", "Chris Ngo"]
journal: "arXiv"
link: "https://arxiv.org/abs/2503.16219"
tags: ["Reinforcement Learning", "Machine Learning", "Large Language Models", "Reasoning"]
---

I’m excited to share a deep dive into "Open-RS," a project born from my recent work with Chris Ngo, detailed in our preprint "Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn’t." Hosted at [https://github.com/knoveleng/open-rs](https://github.com/knoveleng/open-rs), this open-source effort explores how reinforcement learning (RL) can elevate reasoning in small large language models (LLMs) under tight resource constraints.

---

## Introduction

Large language models like GPT-4o and Claude 3.5 Sonnet have set dazzling benchmarks in reasoning, but their scale—often hundreds of billions of parameters—makes them impractical for most outside big tech. Small LLMs (1-10 billion parameters) offer a leaner alternative, yet they struggle with reasoning tasks like math problem-solving without heavy fine-tuning. Our study asks: can RL bridge this gap affordably? Focusing on the 1.5-billion-parameter `DeepSeek-R1-Distill-Qwen-1.5B`, we trained it on 4 NVIDIA A40 GPUs (48 GB VRAM each) in just 24 hours, using a mere 7,000 samples. The results? AMC23 accuracy soared from 63% to 80%, and AIME24 hit 46.7%, topping o1-preview—all for $42. This post unpacks our approach, findings, and why it matters.

---

## The Challenge and Our Approach

### Why Small LLMs?

Small LLMs are lightweight, deployable on modest hardware, and ideal for resource-limited settings—think academic labs or edge devices. But their reasoning often falters compared to giants like DeepSeek-R1 (671B parameters). Traditional methods like supervised fine-tuning (SFT) or massive RL datasets (millions of samples) are costly and slow. We aimed to prove RL could work smarter, not harder.

### Methodology

Our strategy hinged on two pillars:

1. **Curated Dataset**: We built a 39,659-sample math reasoning dataset from `s1` (Muennighoff et al., 2025) and `DeepScaleR` (Luo et al., 2025), filtered for quality using models like Qwen2.5-7B-Instruct. For training, we used a 7,000-sample subset to keep costs low.
2. **GRPO Algorithm**: Adapted from DeepSeek-R1, Group Relative Policy Optimization (GRPO) skips the critic model, cutting compute needs. It optimizes via:
   $$
   \mathcal{J}_{\text{GRPO}}(\theta) = \mathbf{E} \left[ \frac{1}{G} \sum_{i=1}^G \min \left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} A_i, \text{clip} \left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)}, 1-\epsilon, 1+\epsilon \right) A_i \right) - \beta \mathbb{D}_{\text{KL}}(\pi_\theta || \pi_{\text{ref}}) \right]
   $$
   Rewards blend accuracy (correctness in `\boxed{}`), format (`<think>` tags), and a cosine schedule for length control.

Training ran on 4 A40s, limited to 24 hours, with a max length of 4096 or 3584 tokens. We skipped SFT, betting on the base model’s pretraining.

---

## Experiments and Results

We ran three experiments, tweaking data and rewards:

### Experiment 1: High-Quality Data
- **Setup**: 18,615 `open-s1` samples, accuracy + format rewards, 4096-token limit.
- **Results**: AMC23 jumped from 63% to 70%, MATH-500 from 83% to 84% in 50-100 steps. But past 200 steps, accuracy tanked below 60%, with outputs hitting the length cap and drifting to non-English.
- **Insight**: Small LLMs gain fast but falter under prolonged training with strict limits.

### Experiment 2: Mixed Difficulty
- **Setup**: 7,000 samples (3,000 `open-s1`, 3,000 `open-deepscaler`, 1,000 easy), 3584-token limit.
- **Results**: AMC23 hit 80%, MATH-500 85% in 50-100 steps. Post-150 steps, instability returned, with KL divergence spiking.
- **Insight**: Mixing easy and hard problems boosts early gains and brevity, but stability wanes.

### Experiment 3: Cosine Reward
- **Setup**: Same 7,000 samples, cosine reward + English-only prompt, 3584-token limit.
- **Results**: Lengths stabilized (1000-3500 tokens), AMC23 reached 72.5%, MATH-500 84.4%. AIME24 scored 46.7%, beating o1-preview’s 44.6%.
- **Insight**: Cosine rewards tame length, but complex tasks need more room.

### Overall Performance
Our models—Open-RS1 (53.0%), Open-RS2 (55.7%), Open-RS3 (56.3%)—rivaled DeepScaleR-1.5B-Preview (57.0%) and outdid many 7B models, all at $42 vs. thousands for baselines (e.g., $3,629 for DeepScaleR).

![Overall Performance](/images/content/research/20250325_openrs_overall.png)

---

## Why It Matters

Open-RS shows small LLMs can reason competitively with minimal resources. At $42 and 24 hours, it’s a fraction of the cost and time of peers (e.g., Qwen2.5-7B-SimpleRL: $1,633, 36 hours). This opens doors for:
- **Education**: Affordable math tutors on a budget.
- **Edge AI**: Reasoning on devices with limited power.
- **Research**: A scalable framework for low-resource labs.

Our open-source release—code, datasets, models—invites collaboration. Find it at [https://github.com/knoveleng/open-rs](https://github.com/knoveleng/open-rs).

---

## Limitations and Next Steps

We hit hurdles: a 24-hour cap limited training depth, 3584 tokens truncated hard problems, and multilingual drift persisted despite prompts. Future work could:
- Extend training with staged length increases.
- Add language-specific rewards.
- Test non-math domains like coding or science.

---

## Conclusion

Open-RS proves RL can transform small LLMs into reasoning powerhouses, efficiently and affordably. With scores rivaling giants and a $42 price tag, it’s a step toward democratizing AI. Dive in at [https://github.com/knoveleng/open-rs](https://github.com/knoveleng/open-rs), and let’s build on this together.
